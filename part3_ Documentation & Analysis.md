Part 3: Documentation & Analysis
1. Implementation Process
Challenges Encountered:

The primary challenge was handling the large size of the ASVspoof dataset within the limited storage environment (e.g., Google Colab).

This forced a compromise in dataset size â€” only 85 bonafide and 85 spoofed audio samples could be used for training and evaluation.

Additional issues included dimension mismatch errors and Conv1D input formatting, which were resolved through proper reshaping and input pre-processing.

How Challenges Were Addressed:

Reduced dataset size to fit available resources.

Carefully adjusted the input shape fed into the RawNet2 model.

Used light fine-tuning by freezing most of the pre-trained model and only training the final classification layer.

Assumptions Made:

Small subset of data is representative enough to demonstrate the functionality of the approach.

Pre-trained RawNet2 weights are generalizable enough for this specific subset.

2. Analysis
Why RawNet2 Was Selected:

Among the three models evaluated (RawNet2, Wav2Vec2, LCNN), RawNet2 provides a good balance of real-time inference capability, compact model size, and high detection performance.

It's also specifically designed for audio spoofing detection tasks.

How the Model Works (High-Level):

RawNet2 takes raw audio waveform as input.

It uses convolutional layers and residual blocks to learn hierarchical feature representations.

Finally, it passes through fully connected layers for binary classification (bonafide vs. spoofed).

Performance Results on Dataset Subset:

The model was trained on a small, balanced subset (170 audio clips total).

Training over 5 epochs yielded the following results:

Epoch	Loss	Accuracy
1	23.8598	44.85%
2	23.5800	50.00%
3	23.5141	50.74%
4	23.4110	51.47%
5	23.2845	52.94%
Strengths:

Capable of learning directly from raw waveforms.

Effective architecture for audio-based forgery detection.

Weaknesses:

Performance is limited when trained on a small dataset.

Sensitive to input formatting; proper pre-processing is critical.

Lacks data augmentation or regularization in this basic setup.

Suggestions for Future Improvements:

Use full ASVspoof dataset or augment data to increase diversity.

Apply early stopping and learning rate scheduling.

Experiment with partial layer unfreezing for deeper fine-tuning.

Implement validation loop for real-time performance tracking.

3. Reflection Questions
Q1: What were the most significant challenges in implementing this model?

Managing large audio datasets in low-resource environments and handling Conv1D input shape mismatches.

Q2: How might this approach perform in real-world conditions vs. research datasets?

In real-world scenarios, background noise, different accents, and recording quality could reduce performance unless the model is fine-tuned on such diverse data.

Q3: What additional data or resources would improve performance?

Access to the full ASVspoof dataset or other deepfake audio datasets, and more computational resources to train on larger batch sizes and longer epochs.

Q4: How would you approach deploying this model in a production environment?

Optimize the model using quantization or ONNX export, wrap it in a REST API or stream processing pipeline, and monitor real-time performance using user feedback and periodic re-training.


